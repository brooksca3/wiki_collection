import re
import os
import json
import sys
import time
import requests
import random
import argparse

def clean_text(text):
    # Replace all newlines with a unique string
    text = text.replace('\n', '!@#').replace('\t', ' ')
    
    # Split the text by the unique string
    split_text = text.split('!@#')
    
    # Remove elements with 5 or fewer words
    split_text = [segment for segment in split_text if len(segment.split()) > 5]
    
    # Rejoin the text
    text = ' '.join(split_text)
    
    # Remove "References" and "External links" sections
    text = re.sub(r'== References ==.*|== External links ==.*', '', text, flags=re.DOTALL)
    
    return text.strip()

def load_processed_titles(output_file):
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            processed_titles = {line.split(',')[0].strip() for line in f}
        return processed_titles
    return set()

def extract_title_text_and_url(wikipedia_entries):
    titles = []
    texts = []
    urls = []
    for entry in wikipedia_entries:
        title = entry.get('title', '')
        content = entry.get('content', '')
        url = entry.get('url', '')
        cleaned_text = clean_text(content)
        if title and cleaned_text:
            titles.append(title)
            texts.append(cleaned_text)
            urls.append(url)
    return titles, texts, urls

def analyze_text(document_text, retries=3):
    """Send text to the GPTZero API and get analysis results."""
    url = "https://api.gptzero.me/v2/predict/text"
    
    headers = {
        'Content-Type': 'application/json',
        'x-api-key': 'API_KEY'  # Replace with your actual API key
    }
    
    data = {
        "document": document_text,
        "version": "2024-01-09",
        "multilingual": True ## set this to be false if we are doing english wikipedia
    }
    

    for attempt in range(retries):
        response = requests.post(url, headers=headers, json=data)
        
        try:
            response_json = response.json()
            score = response_json['documents'][0]['class_probabilities']['ai']
            return score
        
        except requests.exceptions.JSONDecodeError:
            print(f"JSONDecodeError: Attempt {attempt + 1} failed.")
            print(f"Response content: {response.text}")
            if attempt < retries - 1:
                print("Retrying...")
            else:
                print("Max retries reached. Skipping this entry.")
                return None

        except Exception as e:
            print(f"An error occurred: {e}")
            return None

        # Wait before retrying
        time.sleep(1.5)

    return None

def shuffle_parallel_lists(x):
    titles, texts, urls = x
    combined_list = list(zip(titles, texts, urls))
    random.shuffle(combined_list)
    shuffled_titles, shuffled_texts, shuffled_urls = zip(*combined_list)
    
    return list(shuffled_titles), list(shuffled_texts), list(shuffled_urls)


def process_files(file_path, output_file, max=5000):
    processed_titles = load_processed_titles(output_file)

    with open(file_path, 'r', encoding='utf-8') as file:
        wikipedia_entries = json.load(file)
    
    titles, texts, urls = shuffle_parallel_lists(extract_title_text_and_url(wikipedia_entries))
    texts = [' '.join(text.strip().split()[:150]) for text in texts]
    
    final_texts, final_titles, final_urls = [], [], []
    for ind, text in enumerate(texts):
        if len(text.split()) >= 100:
            final_texts.append(texts[ind])
            final_titles.append(titles[ind])
            final_urls.append(urls[ind])
    
    final_texts = final_texts[:max]
    final_titles = final_titles[:max]
    final_urls = final_urls[:max]

    with open(output_file, 'a', encoding='utf-8') as outfile:
        for title, text, url in zip(final_titles, final_texts, final_urls):
            if title in processed_titles:
                print(f"Skipping already processed title: {title}")
                continue
            
            score_result = analyze_text(text)
            score = score_result
            if score:
                outfile.write(f"{title}, {score}, {url}, actual_text_used_for_scoring: {text}\n")
                outfile.flush()

def main():
    parser = argparse.ArgumentParser(description="Process and score Wikipedia entries.")
    parser.add_argument('--input_file', type=str, required=True, help="Path to the input JSON file in the wiki_jsons directory.")
    ## json file generated by scraping/wikipedia/recent_wiki_scraper.py
    args = parser.parse_args()

    input_file = args.input_file
    print(input_file)
    output_file_name = os.path.basename(input_file).replace('.json', '_scored.json')
    output_file = os.path.join('gptzero_scores', output_file_name)

    os.makedirs('gptzero_scores', exist_ok=True)

    process_files(input_file, output_file)

if __name__ == "__main__":
    main()